{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72b5695",
   "metadata": {},
   "source": [
    "# Construction du classifieur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bbfb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from skimage import io, util\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "# Ah stylé d'avoir trouvé ça !\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e83910",
   "metadata": {},
   "source": [
    "## Récupération des jeux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4782121",
   "metadata": {},
   "source": [
    "## Découper les écocups et les négatifs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f6a18",
   "metadata": {},
   "source": [
    "On découpe les positifs en fonction de bbox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8356c3fc",
   "metadata": {},
   "source": [
    "On va découper des bouts d'images négatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a90eb",
   "metadata": {},
   "source": [
    "## Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11714c0",
   "metadata": {},
   "source": [
    "Supprimer les images vides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09e001b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "pos_patch_fs = [f for f in  os.listdir(os.path.join(\"local_data\", \"4_normalized_patches\", \"pos\")) if f.endswith(\".jpg\")]\n",
    "neg_patch_fs = [f for f in  os.listdir(os.path.join(\"local_data\", \"4_normalized_patches\", \"neg\")) if f.endswith(\".jpg\")]\n",
    "\n",
    "img_pos_train_cut_resized = []\n",
    "img_neg_train_cut_resized = []\n",
    "\n",
    "for f in pos_patch_fs:\n",
    "    try:\n",
    "        patch = plt.imread(\n",
    "            os.path.join(\"local_data\", \"4_normalized_patches\", \"pos\", f)\n",
    "        )\n",
    "        img_pos_train_cut_resized.append(patch[:,:,0]) # le channel grayscale est dupliqué sur les 3 chanaux RGB (on en isole 1)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "for f in neg_patch_fs:\n",
    "    try:\n",
    "        patch = plt.imread(\n",
    "            os.path.join(\"local_data\", \"4_normalized_patches\", \"neg\", f)\n",
    "        )\n",
    "        img_neg_train_cut_resized.append(patch[:,:,0]) # le channel grayscale est dupliqué sur les 3 chanaux RGB (on en isole 1)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a07fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1204\n",
      "6020\n",
      "(128, 72)\n"
     ]
    }
   ],
   "source": [
    "print(len(img_pos_train_cut_resized))\n",
    "print(len(img_neg_train_cut_resized))\n",
    "\n",
    "print(img_pos_train_cut_resized[0].shape) # on est bien en grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516f2da",
   "metadata": {},
   "source": [
    "La commande ci-dessous permet de convertir\n",
    "les valeurs des pixels à valeurs entières [[0; 255]] en valeurs flottantes [0; 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5925075",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pos_train_cut_resized_float = [\n",
    "    util.img_as_float(img) for img in img_pos_train_cut_resized\n",
    "]\n",
    "img_neg_train_cut_resized_float = [\n",
    "    util.img_as_float(img) for img in img_neg_train_cut_resized\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdbf577",
   "metadata": {},
   "source": [
    "On souhaite utiliser directement les pixels comme représentation des images. La commande\n",
    "I.flatten() permet d’aplatir l’image pour obtenir un vecteur\n",
    "\n",
    "TODO : essayer d'autres extracteurs de features (cf. OneNote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bbdc8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pos_train_cut_resized_float_flatten = [\n",
    "    img.flatten() for img in img_pos_train_cut_resized_float\n",
    "]\n",
    "img_neg_train_cut_resized_float_flatten = [\n",
    "    img.flatten() for img in img_neg_train_cut_resized_float\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee6306",
   "metadata": {},
   "source": [
    "## Construire les jeux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50800f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# les cas difficiles ne sont pas inclus -> décision dans \"splitter.py\"\n",
    "# (Et comme je t'ai montré, les cas difficiles sont vraiment dégueulasse, j'ai clean manuellement que les cas faciles)\n",
    "# (Ca sera pas un problème avec l'algo de détection itératif) (cf cours 10)\n",
    "\n",
    "y = []\n",
    "X = img_pos_train_cut_resized_float_flatten + img_neg_train_cut_resized_float_flatten\n",
    "for _ in range(len(img_pos_train_cut_resized_float_flatten)):\n",
    "    y.append(1)\n",
    "\n",
    "for _ in range(len(img_neg_train_cut_resized_float_flatten)):\n",
    "    y.append(0)\n",
    "\n",
    "y_clean = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664c792",
   "metadata": {},
   "source": [
    "#### Créer une pondération pour les exemples difficiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb743c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7296 7296\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90893b80",
   "metadata": {},
   "source": [
    "#### Diviser le jeu\n",
    "\n",
    "TODO : peut-être une validation croisée (déjà implémentée par des bibliothèques, cf. TD6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "653eba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5836\n",
      "5836\n",
      "1460\n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "train_part = 80 / 100\n",
    "train_size = int(len(X) * train_part)\n",
    "\n",
    "# Problème: tout est dans l'ordre, nous devons donc associer chaque image à sa vérité et mélanger les données\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "X_shuffled = np.array(X)[indices]\n",
    "y_shuffled = np.array(y_clean)[indices]\n",
    "\n",
    "\n",
    "X_train = X_shuffled[0:train_size]\n",
    "y_train = y_shuffled[0:train_size]\n",
    "X_validation = X_shuffled[train_size:]\n",
    "y_validation = y_shuffled[train_size:]\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_validation))\n",
    "print(len(y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099bdaf",
   "metadata": {},
   "source": [
    "## Choix du classifieur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae0856",
   "metadata": {},
   "source": [
    "Nous allons choisir le classifieur le plus efficace parmis un certain nombre de classifieurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a43691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    name, model, X_train, y_train, X_validation, y_validation, df_resultat_clf\n",
    "):\n",
    "\n",
    "    print(f\"Testing {name}...\")\n",
    "\n",
    "    # model.fit(X_train, y_train, sample_weight=weight)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_validation)\n",
    "\n",
    "    accuracy = accuracy_score(y_validation, y_pred)\n",
    "\n",
    "    error = (1 - accuracy) * 100\n",
    "    # Calculer le rappel et la précision\n",
    "    rappel = recall_score(y_validation, y_pred)\n",
    "    precision = precision_score(y_validation, y_pred)\n",
    "    f1_score = 2 * (precision * rappel) / (precision + rappel)\n",
    "    avg_precision_score = average_precision_score(y_validation, y_pred)\n",
    "\n",
    "    df_resultat_clf = pd.concat(\n",
    "        [\n",
    "            df_resultat_clf,\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"model\": name,\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"error%\": error,\n",
    "                        \"rappel\": rappel,\n",
    "                        \"precision\": precision,\n",
    "                        \"f1_score\": f1_score,\n",
    "                        \"average_precision_score\": avg_precision_score,\n",
    "                    }\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return df_resultat_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07dcce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_dict():\n",
    "\n",
    "    models_dict = {}\n",
    "    # KNN\n",
    "    neighbors_list = [3, 25, 100, 250]\n",
    "    for neighbors in neighbors_list:\n",
    "        knn = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "        models_dict[f\"KNN (k={neighbors})\"] = knn\n",
    "\n",
    "    # Decision Tree\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    models_dict[\"Decision Tree\"] = decision_tree\n",
    "\n",
    "    # Random Forest\n",
    "    estimators_list = [3, 25, 100, 250]\n",
    "    for estimators in estimators_list:\n",
    "        random_forest = RandomForestClassifier(n_estimators=estimators)\n",
    "        models_dict[f\"Random Forest (n_estimators={estimators})\"] = random_forest\n",
    "\n",
    "    # SVC\n",
    "    svc = SVC()\n",
    "    models_dict[\"SVC\"] = svc\n",
    "\n",
    "    # LinearSVC\n",
    "    linear_svc = LinearSVC()\n",
    "    models_dict[\"Linear SVC\"] = linear_svc\n",
    "\n",
    "    # SVC kernel\n",
    "    svc_kernel_list = [\"poly\", \"rbf\", \"sigmoid\"]\n",
    "    for kernel in svc_kernel_list:\n",
    "        svc_kernel = SVC(kernel=kernel)\n",
    "        models_dict[f\"SVC (kernel={kernel})\"] = svc_kernel\n",
    "\n",
    "    # Logistic Regression\n",
    "    iter_list = [25, 200, 400]\n",
    "    for n_iter in iter_list:\n",
    "        logistic_regression = LogisticRegression(max_iter=n_iter)\n",
    "        models_dict[f\"Logistic Regression (max_iter={n_iter})\"] = logistic_regression\n",
    "\n",
    "    # AdaBoost\n",
    "    estimators_list = [10, 25, 100]\n",
    "    for estimators in estimators_list:\n",
    "        ada_boost = AdaBoostClassifier(n_estimators=estimators)\n",
    "        models_dict[f\"AdaBoost (n_estimators={estimators})\"] = ada_boost\n",
    "\n",
    "    return models_dict\n",
    "\n",
    "    # Gradient Boosting\n",
    "    learning_rate_list = [0.01, 0.2, 0.5]\n",
    "    for learning_rate in learning_rate_list:\n",
    "        gradient_boosting = GradientBoostingClassifier(learning_rate=learning_rate)\n",
    "        models_dict[f\"Gradient Boosting (learning_rate={learning_rate})\"] = (\n",
    "            gradient_boosting\n",
    "        )\n",
    "    return models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae00bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing KNN (k=3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrien\\AppData\\Local\\Temp\\ipykernel_23452\\3502999163.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_resultat_clf = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing KNN (k=25)...\n",
      "Testing KNN (k=100)...\n",
      "Testing KNN (k=250)...\n",
      "Testing Decision Tree...\n",
      "Testing Random Forest (n_estimators=3)...\n",
      "Testing Random Forest (n_estimators=25)...\n",
      "Testing Random Forest (n_estimators=100)...\n",
      "Testing Random Forest (n_estimators=250)...\n",
      "Testing SVC...\n",
      "Testing Linear SVC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dossier_perso\\Projets\\Prog\\SY32_detection_ecocup\\venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Dossier_perso\\Projets\\Prog\\SY32_detection_ecocup\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SVC (kernel=poly)...\n",
      "Testing SVC (kernel=rbf)...\n",
      "Testing SVC (kernel=sigmoid)...\n",
      "Testing Logistic Regression (max_iter=25)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dossier_perso\\Projets\\Prog\\SY32_detection_ecocup\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Logistic Regression (max_iter=200)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dossier_perso\\Projets\\Prog\\SY32_detection_ecocup\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Logistic Regression (max_iter=400)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dossier_perso\\Projets\\Prog\\SY32_detection_ecocup\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Dossier_perso\\Projets\\Prog\\SY32_detection_ecocup\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AdaBoost (n_estimators=10)...\n",
      "Testing AdaBoost (n_estimators=25)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dossier_perso\\Projets\\Prog\\SY32_detection_ecocup\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing AdaBoost (n_estimators=100)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dossier_perso\\Projets\\Prog\\SY32_detection_ecocup\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 model  accuracy     error%    rappel  \\\n",
      "9                                  SVC  0.922603   7.739726  0.547325   \n",
      "12                    SVC (kernel=rbf)  0.922603   7.739726  0.547325   \n",
      "11                   SVC (kernel=poly)  0.916438   8.356164  0.654321   \n",
      "8     Random Forest (n_estimators=250)  0.909589   9.041096  0.460905   \n",
      "7     Random Forest (n_estimators=100)  0.909589   9.041096  0.469136   \n",
      "6      Random Forest (n_estimators=25)  0.909589   9.041096  0.473251   \n",
      "19         AdaBoost (n_estimators=100)  0.902055   9.794521  0.551440   \n",
      "14   Logistic Regression (max_iter=25)  0.896575  10.342466  0.547325   \n",
      "0                            KNN (k=3)  0.888356  11.164384  0.386831   \n",
      "15  Logistic Regression (max_iter=200)  0.884247  11.575342  0.592593   \n",
      "18          AdaBoost (n_estimators=25)  0.870548  12.945205  0.320988   \n",
      "16  Logistic Regression (max_iter=400)  0.863014  13.698630  0.572016   \n",
      "17          AdaBoost (n_estimators=10)  0.863014  13.698630  0.230453   \n",
      "5       Random Forest (n_estimators=3)  0.863014  13.698630  0.497942   \n",
      "1                           KNN (k=25)  0.856849  14.315068  0.139918   \n",
      "10                          Linear SVC  0.843151  15.684932  0.576132   \n",
      "2                          KNN (k=100)  0.839041  16.095890  0.032922   \n",
      "3                          KNN (k=250)  0.834247  16.575342  0.004115   \n",
      "4                        Decision Tree  0.823973  17.602740  0.543210   \n",
      "13                SVC (kernel=sigmoid)  0.807534  19.246575  0.057613   \n",
      "\n",
      "    precision  f1_score  average_precision_score  \n",
      "9    0.977941  0.701847                 0.610594  \n",
      "12   0.977941  0.701847                 0.610594  \n",
      "11   0.807107  0.722727                 0.585641  \n",
      "8    0.991150  0.629213                 0.546553  \n",
      "7    0.974359  0.633333                 0.545463  \n",
      "6    0.966387  0.635359                 0.545015  \n",
      "19   0.797619  0.652068                 0.514497  \n",
      "14   0.764368  0.637890                 0.493700  \n",
      "0    0.870370  0.535613                 0.438741  \n",
      "15   0.672897  0.630197                 0.466562  \n",
      "18   0.764706  0.452174                 0.358475  \n",
      "16   0.591489  0.581590                 0.409575  \n",
      "17   0.811594  0.358974                 0.315116  \n",
      "5    0.608040  0.547511                 0.386331  \n",
      "1    1.000000  0.245487                 0.283068  \n",
      "10   0.526316  0.550098                 0.373775  \n",
      "2    1.000000  0.063745                 0.193881  \n",
      "3    1.000000  0.008197                 0.169869  \n",
      "4    0.474820  0.506718                 0.333954  \n",
      "13   0.212121  0.090615                 0.169070  \n"
     ]
    }
   ],
   "source": [
    "df_resultat_clf = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"model\",\n",
    "        \"accuracy\",\n",
    "        \"error%\",\n",
    "        \"rappel\",\n",
    "        \"precision\",\n",
    "        \"f1_score\",\n",
    "        \"average_precision_score\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "models_dict = get_models_dict()\n",
    "\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    df_resultat_clf = test_model(\n",
    "        name, model, X_train, y_train, X_validation, y_validation, df_resultat_clf\n",
    "    )\n",
    "\n",
    "df_resultat_clf = df_resultat_clf.sort_values(by=\"error%\")\n",
    "print(df_resultat_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb69e912",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_resultat_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_resultat_clf\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_resultat_clf' is not defined"
     ]
    }
   ],
   "source": [
    "df_resultat_clf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bda3f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Accuracy:\n",
      "                                    model  accuracy\n",
      "23  Gradient Boosting (learning_rate=0.3)  0.747312\n",
      "8        Random Forest (n_estimators=250)  0.741935\n",
      "7        Random Forest (n_estimators=100)  0.741935\n",
      "\n",
      "Top 3 Rappel:\n",
      "               model    rappel\n",
      "3        KNN (k=250)  0.862745\n",
      "9                SVC  0.843137\n",
      "12  SVC (kernel=rbf)  0.843137\n",
      "\n",
      "Top 3 Précision:\n",
      "                               model  precision\n",
      "0                          KNN (k=3)   0.816667\n",
      "11                 SVC (kernel=poly)   0.784946\n",
      "7   Random Forest (n_estimators=100)   0.764706\n",
      "\n",
      "Top 3 F1 Score:\n",
      "                                    model  f1_score\n",
      "8        Random Forest (n_estimators=250)  0.775701\n",
      "23  Gradient Boosting (learning_rate=0.3)  0.775120\n",
      "5          Random Forest (n_estimators=3)  0.766355\n",
      "\n",
      "Top 3 Average Precision Score:\n",
      "                                    model  average_precision_score\n",
      "11                      SVC (kernel=poly)                 0.717689\n",
      "23  Gradient Boosting (learning_rate=0.3)                 0.714058\n",
      "7        Random Forest (n_estimators=100)                 0.713807\n"
     ]
    }
   ],
   "source": [
    "## Affichons les 3 meilleurs résultats pour chaque score\n",
    "# accuracy\n",
    "top_accuracy = df_resultat_clf.nlargest(3, \"accuracy\")\n",
    "print(\"Top 3 Accuracy:\")\n",
    "print(top_accuracy[[\"model\", \"accuracy\"]])\n",
    "\n",
    "# rappel\n",
    "top_recall = df_resultat_clf.nlargest(3, \"rappel\")\n",
    "print(\"\\nTop 3 Rappel:\")\n",
    "print(top_recall[[\"model\", \"rappel\"]])\n",
    "\n",
    "# précision\n",
    "top_precision = df_resultat_clf.nlargest(3, \"precision\")\n",
    "print(\"\\nTop 3 Précision:\")\n",
    "print(top_precision[[\"model\", \"precision\"]])\n",
    "\n",
    "# f1_score\n",
    "top_f1_score = df_resultat_clf.nlargest(3, \"f1_score\")\n",
    "print(\"\\nTop 3 F1 Score:\")\n",
    "print(top_f1_score[[\"model\", \"f1_score\"]])\n",
    "\n",
    "# average_precision_score\n",
    "top_avg_precision = df_resultat_clf.nlargest(3, \"average_precision_score\")\n",
    "print(\"\\nTop 3 Average Precision Score:\")\n",
    "print(top_avg_precision[[\"model\", \"average_precision_score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6799888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enregistrer le dataframe\n",
    "df_resultat_clf.to_csv(\"resultats_classification.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
